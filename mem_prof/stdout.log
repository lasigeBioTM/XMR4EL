running k-means on cpu..
[running kmeans]: 0it [00:00, ?it/s][running kmeans]: 0it [00:00, ?it/s, center_shift=3.680442, iteration=1, tol=0.000100][running kmeans]: 1it [00:00, 87.64it/s, center_shift=0.001594, iteration=2, tol=0.000100][running kmeans]: 2it [00:00, 142.28it/s, center_shift=0.000778, iteration=3, tol=0.000100][running kmeans]: 3it [00:00, 179.80it/s, center_shift=0.000800, iteration=4, tol=0.000100][running kmeans]: 4it [00:00, 205.98it/s, center_shift=0.000799, iteration=5, tol=0.000100][running kmeans]: 5it [00:00, 225.79it/s, center_shift=0.000000, iteration=6, tol=0.000100][running kmeans]: 6it [00:00, 254.37it/s, center_shift=0.000000, iteration=6, tol=0.000100]
Fusing Scores
Filename: /app/xmr4el/xmr/base.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   358    909.3 MiB    909.3 MiB           1       @profile
   359                                             def train(self, X_train, Y_train, Z_train, local_to_global, global_to_local):
   360                                                 """
   361                                                     X_train: X_processed
   362                                                     Y_train, Y_binazier
   363                                                     Z, Pifa embeddings
   364                                                 """
   365                                                 
   366                                                 # --- Ensure Z is in fused space ---
   367    909.3 MiB      0.0 MiB           1           Z_train = normalize(Z_train, norm="l2", axis=1) 
   368    909.3 MiB      0.0 MiB           1           self.label_embeddings = Z_train
   369    909.3 MiB      0.0 MiB           1           del Z_train
   370                                                 
   371    909.3 MiB      0.0 MiB           1           self.global_to_local_idx = global_to_local
   372    909.3 MiB      0.0 MiB           1           self.local_to_global_idx = np.array(local_to_global, dtype=int)
   373                                                 
   374    909.3 MiB      0.0 MiB           1           del global_to_local
   375                                                 
   376                                                 # print("Clustering")
   377                                                 # Make the Clustering
   378    909.3 MiB      0.0 MiB           1           cluster_model = Clustering()
   379    913.5 MiB      4.2 MiB           2           cluster_model.train(Z=self.label_embeddings, 
   380    909.3 MiB      0.0 MiB           1                               local_to_global_idx=self.local_to_global_idx,
   381    909.3 MiB      0.0 MiB           1                               min_leaf_size=self.min_leaf_size,
   382    909.3 MiB      0.0 MiB           1                               max_leaf_size=self.max_leaf_size,
   383    909.3 MiB      0.0 MiB           1                               clustering_config=self.clustering_config,
   384    909.3 MiB      0.0 MiB           1                               dtype=np.float32
   385                                                                     ) # Hardcoded
   386                                                 
   387    913.5 MiB      0.0 MiB           1           if cluster_model.is_empty:
   388                                                     return
   389                                                 
   390    913.5 MiB      0.0 MiB           1           self.cluster_model = cluster_model
   391    913.5 MiB      0.0 MiB           1           del cluster_model
   392    913.5 MiB      0.0 MiB           1           gc.collect()
   393                                                 
   394                                                 # Retrieve C
   395    913.5 MiB      0.0 MiB           1           C = self.cluster_model.c_node
   396    913.5 MiB      0.0 MiB           1           cluster_labels = np.asarray(C.argmax(axis=1)).flatten()
   397                                             
   398                                                 # Make the Matcher
   399    913.5 MiB      0.0 MiB           1           matcher_model = Matcher()  
   400    913.5 MiB    -96.4 MiB           2           matcher_model.train(X_train, 
   401    913.5 MiB      0.0 MiB           1                               Y_train, 
   402    913.5 MiB      0.0 MiB           1                               local_to_global_idx=self.local_to_global_idx, 
   403    913.5 MiB      0.0 MiB           1                               global_to_local_idx=self.global_to_local_idx, 
   404    913.5 MiB      0.0 MiB           1                               C=C,
   405    913.5 MiB      0.0 MiB           1                               matcher_config=self.matcher_config,
   406    913.5 MiB      0.0 MiB           1                               dtype=np.float32
   407                                                                     )     
   408                                                  
   409    817.2 MiB    -96.4 MiB           1           self.matcher_model = matcher_model 
   410    817.2 MiB      0.0 MiB           1           del matcher_model
   411    821.5 MiB      4.3 MiB           1           gc.collect()
   412                                                 
   413    821.5 MiB      0.0 MiB           1           train_ranker = self.ranker_every_layer or self.is_last_layer
   414                                                 
   415    821.5 MiB      0.0 MiB           1           def _topb_sparse(P: np.ndarray, b: int) -> csr_matrix:
   416                                                     # P: (n x K_or_L) dense proba; returns (n x K_or_L) CSR 0/1 mask of top-b per row
   417                                                     n, K = P.shape
   418                                                     b = max(1, min(b, K))
   419                                                     idx_part = np.argpartition(P, K - b, axis=1)[:, -b:]
   420                                                     rows = np.repeat(np.arange(n, dtype=np.int32), b)
   421                                                     cols = idx_part.ravel()
   422                                                     data = np.ones(n * b, dtype=np.int8)
   423                                                     return csr_matrix((data, (rows, cols)), shape=(n, K))
   424                                                 
   425    821.5 MiB      0.0 MiB           1           if train_ranker:
   426                                                 
   427    821.5 MiB      0.0 MiB           1               M_TFN = self.matcher_model.m_node
   428    821.5 MiB      0.0 MiB           1               M_MAN = None
   429                                                 
   430    821.5 MiB      0.0 MiB           1               if self.is_last_layer:
   431                                                         P = self.matcher_model.predict_proba(X_train)
   432                                                         M_MAN = _topb_sparse(P, b=5)
   433                                                     
   434                                                     # print("Ranker")
   435    821.5 MiB      0.0 MiB           1               ranker_model = Ranker()
   436   1460.5 MiB    639.0 MiB           2               ranker_model.train(X_train, 
   437    821.5 MiB      0.0 MiB           1                                   Y_train, 
   438    821.5 MiB      0.0 MiB           1                                   self.label_embeddings, 
   439    821.5 MiB      0.0 MiB           1                                   M_TFN, 
   440    821.5 MiB      0.0 MiB           1                                   M_MAN, 
   441    821.5 MiB      0.0 MiB           1                                   cluster_labels,
   442    821.5 MiB      0.0 MiB           1                                   local_to_global_idx=self.local_to_global_idx,
   443    821.5 MiB      0.0 MiB           1                                   layer=self.layer,
   444    821.5 MiB      0.0 MiB           1                                   n_label_workers=self.n_workers,
   445    821.5 MiB      0.0 MiB           1                                   ranker_config=self.ranker_config,
   446    821.5 MiB      0.0 MiB           1                                   cur_config=self.cur_config
   447                                                                         )
   448                                                     
   449   1458.9 MiB     -1.5 MiB           1               self.ranker_model = ranker_model
   450   1458.9 MiB      0.0 MiB           1               del ranker_model
   451                                                 else:
   452                                                     self.ranker_model = None
   453                                                     
   454   1458.9 MiB      0.0 MiB           1           gc.collect()
   455                                                 
   456   1458.9 MiB      0.0 MiB           1           print("Fusing Scores")
   457   1458.9 MiB      0.0 MiB           1           if not self.is_last_layer:
   458   1459.1 MiB      0.1 MiB           1               cluster_scores = self.matcher_model.predict_proba(X_train)
   459   1459.1 MiB      0.0 MiB           1               self.fused_scores = csr_matrix(np.maximum(cluster_scores, 0.0))
   460                                                 else:
   461                                                     I_L = sp_eye(self.label_embeddings.shape[0], format="csr", dtype=np.float32)
   462                                                     self.fused_scores = self.fused_predict(X_train, self.label_embeddings, I_L, alpha=0.5, fusion="lp_hinge", p=3)


Filename: /app/xmr4el/xmr/base.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   749   1459.1 MiB   1459.1 MiB           1       @profile
   750                                             def prepare_layer(self, X, Y, Z, C, fused_scores, local_to_global_idx):
   751                                                 """
   752                                                 Returns a list of tuples, one per (non-empty) cluster c:
   753                                                 (X_aug, Y_node, Z_node_aug, local_to_global_next, global_to_local_next, c)
   754                                                 where `c` is the *cluster id* in the parent.
   755                                                 """
   756   1459.1 MiB      0.0 MiB           1           K_next = C.shape[1]
   757   1459.1 MiB      0.0 MiB           1           inputs = []
   758   1459.1 MiB      0.0 MiB           1           fused_dense = fused_scores.toarray() if hasattr(fused_scores, "toarray") else np.asarray(fused_scores)
   759                                         
   760   1840.9 MiB      0.0 MiB           3           for c in range(K_next):
   761   1637.1 MiB      0.0 MiB           2               local_idxs = C[:, c].nonzero()[0]
   762   1637.1 MiB      0.0 MiB           2               if len(local_idxs) == 0:
   763                                                         continue
   764                                         
   765   1637.1 MiB      0.0 MiB           2               local_to_global_next = local_to_global_idx[local_idxs]
   766   1637.1 MiB      0.0 MiB         202               global_to_local_next = {g: i for i, g in enumerate(local_to_global_next)}
   767                                         
   768   1637.1 MiB      0.2 MiB           2               Y_sub = Y[:, local_idxs]
   769   1637.1 MiB      0.0 MiB           2               mention_mask = (Y_sub.sum(axis=1).A1 > 0)
   770                                         
   771   1718.6 MiB    135.9 MiB           2               X_node = X[mention_mask]
   772   1718.6 MiB      0.0 MiB           2               Y_node = Y_sub[mention_mask, :]
   773   1718.6 MiB      0.0 MiB           2               if X_node.shape[0] == 0:
   774                                                         continue
   775                                         
   776   1718.6 MiB      0.0 MiB           2               Z_node_base = Z[local_idxs, :]
   777                                         
   778   1718.6 MiB      0.0 MiB           2               fused_c = fused_dense[mention_mask, :]
   779   1718.6 MiB      0.0 MiB           2               feat_c = fused_c[:, c].ravel()
   780   1718.6 MiB      0.0 MiB           2               feat_sum = fused_c.sum(axis=1).ravel()
   781   1718.6 MiB      0.0 MiB           2               feat_max = fused_c.max(axis=1).ravel()
   782   1718.6 MiB      0.0 MiB           2               feat_node = np.vstack([feat_c, feat_sum, feat_max]).T
   783   1718.6 MiB      0.0 MiB           2               sparse_feats = csr_matrix(feat_node)
   784                                         
   785   1800.1 MiB    202.1 MiB           2               X_aug = hstack([X_node, sparse_feats], format="csr")
   786   1840.9 MiB     41.1 MiB           2               X_aug = normalize(X_aug, norm="l2", axis=1)
   787                                         
   788   1840.9 MiB      0.0 MiB           2               try:
   789   1840.9 MiB      0.0 MiB           2                   X_node_dense = X_node.toarray()
   790                                                     except Exception:
   791                                                         X_node_dense = np.asarray(X_node)
   792                                         
   793   1840.9 MiB      0.0 MiB           2               if X_node_dense.size == 0 or Z_node_base.size == 0:
   794                                                         label_feats = np.zeros((Z_node_base.shape[0], 3), dtype=Z_node_base.dtype)
   795                                                     else:
   796   1840.9 MiB      2.3 MiB           2                   scores_mat = X_node_dense.dot(Z_node_base.T)
   797   1840.9 MiB      0.0 MiB           2                   mean_per_label = scores_mat.mean(axis=0)
   798   1840.9 MiB      0.0 MiB           2                   sum_per_label = scores_mat.sum(axis=0)
   799   1840.9 MiB      0.0 MiB           2                   max_per_label = scores_mat.max(axis=0)
   800   1840.9 MiB      0.0 MiB           2                   label_feats = np.vstack([mean_per_label, sum_per_label, max_per_label]).T
   801                                         
   802   1840.9 MiB      0.0 MiB           2               Z_node_aug = np.hstack([Z_node_base, label_feats])
   803   1840.9 MiB      0.4 MiB           2               Z_node_aug = normalize(Z_node_aug, norm="l2", axis=1)
   804                                         
   805   1840.9 MiB      0.0 MiB           2               inputs.append((X_aug, Y_node, Z_node_aug, local_to_global_next, global_to_local_next, c))
   806                                         
   807   1840.9 MiB      0.0 MiB           1           return inputs


running k-means on cpu..
[running kmeans]: 0it [00:00, ?it/s][running kmeans]: 0it [00:00, ?it/s, center_shift=0.001928, iteration=1, tol=0.000100][running kmeans]: 1it [00:00, 26.62it/s, center_shift=0.000006, iteration=2, tol=0.000100][running kmeans]: 2it [00:00, 51.27it/s, center_shift=0.000006, iteration=2, tol=0.000100]
Fusing Scores
Filename: /app/xmr4el/xmr/base.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   243   1833.8 MiB   1833.8 MiB           1       @profile
   244                                             def fused_predict(self, X, Z, C, alpha=0.5, batch_size=32768,
   245                                                               fusion: str = "lp_hinge", p: int = 3):
   246                                                 """Batched matcher/ranker fusion.
   247                                         
   248                                                 Parameters
   249                                                 ----------
   250                                                 X : array-like or sparse matrix
   251                                                     Query feature matrix.
   252                                                 Z : array-like or sparse matrix
   253                                                     Label embedding matrix in the fused space.
   254                                                 C : csr_matrix
   255                                                     Cluster assignment matrix for projecting label scores.
   256                                                 alpha : float, optional
   257                                                     Interpolation coefficient between matcher and ranker scores.
   258                                                 batch_size : int, optional
   259                                                     Number of (query, label) pairs processed per ranker batch.
   260                                                 fusion : {"geometric", "lp_hinge"}, optional
   261                                                     Fusion strategy. "geometric" uses the geometric mean of matcher and
   262                                                     ranker scores. "lp_hinge" performs an L-p interpolation with a
   263                                                     hinge at zero.
   264                                                 p : int, optional
   265                                                     The ``p`` parameter used when ``fusion="lp_hinge"``.
   266                                         
   267                                                 Returns
   268                                                 -------
   269                                                 csr_matrix
   270                                                     Fused cluster score matrix of shape ``(n_queries, n_clusters)``.
   271                                                 """
   272   1833.8 MiB      0.0 MiB           1           N = X.shape[0]
   273   1833.8 MiB      0.0 MiB           1           L_local = Z.shape[0]
   274                                         
   275                                                 # --- matcher (local label-level) scores ---
   276   1833.8 MiB      0.0 MiB           1           matcher_scores = csr_matrix(self.matcher_model.model.predict_proba(X), dtype=np.float32)
   277                                         
   278                                                 # --- flatten mention-local label pairs ---
   279   1833.8 MiB      0.0 MiB           1           rows_list, cols_list, matcher_flat = [], [], []
   280   1833.8 MiB      0.0 MiB        5246           for i in range(N):
   281   1833.8 MiB      0.0 MiB        5245               row = matcher_scores.getrow(i)
   282   1833.8 MiB      0.0 MiB        5245               local_idxs, scores = row.indices, row.data
   283   1833.8 MiB      0.0 MiB        5245               rows_list.extend([i] * len(local_idxs))
   284   1833.8 MiB      0.0 MiB        5245               cols_list.extend(local_idxs)
   285   1833.8 MiB      0.0 MiB        5245               matcher_flat.extend(scores)
   286                                         
   287   1833.8 MiB      0.0 MiB           1           rows_list = np.array(rows_list, dtype=np.int32)
   288   1833.8 MiB      0.0 MiB           1           cols_list = np.array(cols_list, dtype=np.int32)
   289   1833.8 MiB      0.0 MiB           1           matcher_flat = np.array(matcher_flat, dtype=np.float32)
   290                                         
   291   1833.8 MiB      0.0 MiB           1           ranker_score = np.ones(len(rows_list), dtype=np.float32)
   292                                         
   293   1833.8 MiB      0.0 MiB           1           if self.ranker_model:
   294                                                     # Transfer original mention embeddings to dense once
   295   1833.8 MiB      0.0 MiB           1               X_dense = X.toarray() if hasattr(X, "toarray") else np.asarray(X)
   296   1833.8 MiB      0.0 MiB       10491               global_idxs = np.array([self.local_to_global_idx[i] for i in cols_list])
   297                                         
   298                                                     # print(self.ranker_model.model_dict)
   299                                         
   300                                                     # detect loss type
   301   1833.8 MiB      0.0 MiB           1               first_model = next(iter(self.ranker_model.model_dict.values()))
   302   1833.8 MiB      0.0 MiB           2               is_hinge = first_model.config.get("type") == "sklearnsgdclassifier" and \
   303   1833.8 MiB      0.0 MiB           1                       first_model.config.get("kwargs", {}).get("loss") == "hinge"
   304                                         
   305   1833.8 MiB      0.0 MiB           1               num_pairs = len(rows_list)
   306   2375.3 MiB      0.0 MiB           2               for start in range(0, num_pairs, batch_size):
   307   1833.8 MiB      0.0 MiB           1                   end = min(start + batch_size, num_pairs)
   308                                         
   309   1833.8 MiB      0.0 MiB           1                   b_rows = rows_list[start:end]
   310   1833.8 MiB      0.0 MiB           1                   b_cols = cols_list[start:end]
   311   1833.8 MiB      0.0 MiB           1                   b_global = global_idxs[start:end]
   312                                         
   313                                                         # build concatenated embeddings only for this small batch
   314   1954.1 MiB    120.2 MiB           1                   X_part = X_dense[b_rows]
   315   2014.2 MiB     60.2 MiB           1                   Z_part = Z[b_cols]
   316   2254.8 MiB    240.5 MiB           1                   batch_inp = np.hstack([X_part, Z_part])
   317                                         
   318   2254.8 MiB      0.0 MiB           1                   scores_batch = np.ones(end - start, dtype=np.float32)
   319                                                         # group by global label in this batch
   320   2375.3 MiB      0.0 MiB           3                   for g in np.unique(b_global):
   321   2375.2 MiB      0.0 MiB           2                       mask = (b_global == g)
   322   2375.2 MiB      0.0 MiB           2                       if g not in self.ranker_model.model_dict:
   323                                                                 continue
   324   2375.2 MiB      0.0 MiB           2                       mdl = self.ranker_model.model_dict[g]
   325   2375.3 MiB    120.4 MiB           2                       sub_inp = batch_inp[mask]
   326                                         
   327   2375.3 MiB      0.0 MiB           2                       try:
   328   2375.3 MiB      0.0 MiB           2                           if is_hinge:
   329   2375.3 MiB      0.2 MiB           2                               score = np.clip(expit(mdl.decision_function(sub_inp)), 1e-6, 1.0)
   330                                                                 else:
   331                                                                     score = np.clip(mdl.predict_proba(sub_inp)[:, 1], 1e-6, 1.0)
   332   2375.3 MiB      0.0 MiB           2                           scores_batch[mask] = score
   333                                                                 
   334                                                             except Exception:
   335                                                                 scores_batch[mask] = 1.0
   336                                         
   337   2375.3 MiB      0.0 MiB           1                   ranker_score[start:end] = scores_batch
   338                                                         
   339                                                     else:
   340   2375.3 MiB      0.0 MiB           1                   alpha = 0
   341                                                         
   342   2375.3 MiB      0.0 MiB           1           self.alpha = alpha
   343                                         
   344   2375.3 MiB      0.0 MiB           1           if fusion == "lp_hinge":
   345   2375.3 MiB      0.0 MiB           3               fused = ((1 - self.alpha) * (matcher_flat ** p) +
   346   2375.3 MiB      0.0 MiB           2                        self.alpha * (ranker_score ** p)) ** (1.0 / p)
   347   2375.3 MiB      0.0 MiB           1               fused = np.maximum(fused, 0.0)
   348                                                 else:
   349                                                     fused = (matcher_flat ** (1 - self.alpha)) * (ranker_score ** self.alpha)
   350                                         
   351                                                 # --- build local-label fused matrix ---
   352   2375.3 MiB      0.0 MiB           1           entity_fused = csr_matrix((fused, (rows_list, cols_list)), shape=(N, L_local), dtype=np.float32)
   353                                         
   354                                                 # --- project to clusters ---
   355   2375.3 MiB      0.0 MiB           1           cluster_fused = entity_fused.dot(C)
   356   2375.3 MiB      0.0 MiB           1           return csr_matrix(cluster_fused)


Filename: /app/xmr4el/xmr/base.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   358   1760.2 MiB   1760.2 MiB           1       @profile
   359                                             def train(self, X_train, Y_train, Z_train, local_to_global, global_to_local):
   360                                                 """
   361                                                     X_train: X_processed
   362                                                     Y_train, Y_binazier
   363                                                     Z, Pifa embeddings
   364                                                 """
   365                                                 
   366                                                 # --- Ensure Z is in fused space ---
   367   1760.2 MiB      0.0 MiB           1           Z_train = normalize(Z_train, norm="l2", axis=1) 
   368   1760.2 MiB      0.0 MiB           1           self.label_embeddings = Z_train
   369   1760.2 MiB      0.0 MiB           1           del Z_train
   370                                                 
   371   1760.2 MiB      0.0 MiB           1           self.global_to_local_idx = global_to_local
   372   1760.2 MiB      0.0 MiB           1           self.local_to_global_idx = np.array(local_to_global, dtype=int)
   373                                                 
   374   1760.2 MiB      0.0 MiB           1           del global_to_local
   375                                                 
   376                                                 # print("Clustering")
   377                                                 # Make the Clustering
   378   1760.2 MiB      0.0 MiB           1           cluster_model = Clustering()
   379   1774.3 MiB     14.1 MiB           2           cluster_model.train(Z=self.label_embeddings, 
   380   1760.2 MiB      0.0 MiB           1                               local_to_global_idx=self.local_to_global_idx,
   381   1760.2 MiB      0.0 MiB           1                               min_leaf_size=self.min_leaf_size,
   382   1760.2 MiB      0.0 MiB           1                               max_leaf_size=self.max_leaf_size,
   383   1760.2 MiB      0.0 MiB           1                               clustering_config=self.clustering_config,
   384   1760.2 MiB      0.0 MiB           1                               dtype=np.float32
   385                                                                     ) # Hardcoded
   386                                                 
   387   1774.3 MiB      0.0 MiB           1           if cluster_model.is_empty:
   388                                                     return
   389                                                 
   390   1774.3 MiB      0.0 MiB           1           self.cluster_model = cluster_model
   391   1774.3 MiB      0.0 MiB           1           del cluster_model
   392   1774.3 MiB      0.0 MiB           1           gc.collect()
   393                                                 
   394                                                 # Retrieve C
   395   1774.3 MiB      0.0 MiB           1           C = self.cluster_model.c_node
   396   1774.3 MiB      0.0 MiB           1           cluster_labels = np.asarray(C.argmax(axis=1)).flatten()
   397                                             
   398                                                 # Make the Matcher
   399   1774.3 MiB      0.0 MiB           1           matcher_model = Matcher()  
   400   1774.3 MiB    -21.8 MiB           2           matcher_model.train(X_train, 
   401   1774.3 MiB      0.0 MiB           1                               Y_train, 
   402   1774.3 MiB      0.0 MiB           1                               local_to_global_idx=self.local_to_global_idx, 
   403   1774.3 MiB      0.0 MiB           1                               global_to_local_idx=self.global_to_local_idx, 
   404   1774.3 MiB      0.0 MiB           1                               C=C,
   405   1774.3 MiB      0.0 MiB           1                               matcher_config=self.matcher_config,
   406   1774.3 MiB      0.0 MiB           1                               dtype=np.float32
   407                                                                     )     
   408                                                  
   409   1752.5 MiB    -21.8 MiB           1           self.matcher_model = matcher_model 
   410   1752.5 MiB      0.0 MiB           1           del matcher_model
   411   1756.8 MiB      4.3 MiB           1           gc.collect()
   412                                                 
   413   1756.8 MiB      0.0 MiB           1           train_ranker = self.ranker_every_layer or self.is_last_layer
   414                                                 
   415   1757.3 MiB      0.0 MiB           2           def _topb_sparse(P: np.ndarray, b: int) -> csr_matrix:
   416                                                     # P: (n x K_or_L) dense proba; returns (n x K_or_L) CSR 0/1 mask of top-b per row
   417   1757.3 MiB      0.0 MiB           1               n, K = P.shape
   418   1757.3 MiB      0.0 MiB           1               b = max(1, min(b, K))
   419   1757.3 MiB      0.0 MiB           1               idx_part = np.argpartition(P, K - b, axis=1)[:, -b:]
   420   1757.3 MiB      0.0 MiB           1               rows = np.repeat(np.arange(n, dtype=np.int32), b)
   421   1757.3 MiB      0.0 MiB           1               cols = idx_part.ravel()
   422   1757.3 MiB      0.0 MiB           1               data = np.ones(n * b, dtype=np.int8)
   423   1757.7 MiB      0.4 MiB           1               return csr_matrix((data, (rows, cols)), shape=(n, K))
   424                                                 
   425   1756.8 MiB      0.0 MiB           1           if train_ranker:
   426                                                 
   427   1756.8 MiB      0.0 MiB           1               M_TFN = self.matcher_model.m_node
   428   1756.8 MiB      0.0 MiB           1               M_MAN = None
   429                                                 
   430   1756.8 MiB      0.0 MiB           1               if self.is_last_layer:
   431   1757.3 MiB      0.5 MiB           1                   P = self.matcher_model.predict_proba(X_train)
   432   1757.7 MiB      0.0 MiB           1                   M_MAN = _topb_sparse(P, b=5)
   433                                                     
   434                                                     # print("Ranker")
   435   1757.7 MiB      0.0 MiB           1               ranker_model = Ranker()
   436   1833.8 MiB     76.2 MiB           2               ranker_model.train(X_train, 
   437   1757.7 MiB      0.0 MiB           1                                   Y_train, 
   438   1757.7 MiB      0.0 MiB           1                                   self.label_embeddings, 
   439   1757.7 MiB      0.0 MiB           1                                   M_TFN, 
   440   1757.7 MiB      0.0 MiB           1                                   M_MAN, 
   441   1757.7 MiB      0.0 MiB           1                                   cluster_labels,
   442   1757.7 MiB      0.0 MiB           1                                   local_to_global_idx=self.local_to_global_idx,
   443   1757.7 MiB      0.0 MiB           1                                   layer=self.layer,
   444   1757.7 MiB      0.0 MiB           1                                   n_label_workers=self.n_workers,
   445   1757.7 MiB      0.0 MiB           1                                   ranker_config=self.ranker_config,
   446   1757.7 MiB      0.0 MiB           1                                   cur_config=self.cur_config
   447                                                                         )
   448                                                     
   449   1833.8 MiB      0.0 MiB           1               self.ranker_model = ranker_model
   450   1833.8 MiB      0.0 MiB           1               del ranker_model
   451                                                 else:
   452                                                     self.ranker_model = None
   453                                                     
   454   1833.8 MiB      0.0 MiB           1           gc.collect()
   455                                                 
   456   1833.8 MiB      0.0 MiB           1           print("Fusing Scores")
   457   1833.8 MiB      0.0 MiB           1           if not self.is_last_layer:
   458                                                     cluster_scores = self.matcher_model.predict_proba(X_train)
   459                                                     self.fused_scores = csr_matrix(np.maximum(cluster_scores, 0.0))
   460                                                 else:
   461   1833.8 MiB      0.0 MiB           1               I_L = sp_eye(self.label_embeddings.shape[0], format="csr", dtype=np.float32)
   462   1834.0 MiB      0.1 MiB           1               self.fused_scores = self.fused_predict(X_train, self.label_embeddings, I_L, alpha=0.5, fusion="lp_hinge", p=3)


Filename: /app/xmr4el/xmr/base.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   749   1834.0 MiB   1834.0 MiB           1       @profile
   750                                             def prepare_layer(self, X, Y, Z, C, fused_scores, local_to_global_idx):
   751                                                 """
   752                                                 Returns a list of tuples, one per (non-empty) cluster c:
   753                                                 (X_aug, Y_node, Z_node_aug, local_to_global_next, global_to_local_next, c)
   754                                                 where `c` is the *cluster id* in the parent.
   755                                                 """
   756   1834.0 MiB      0.0 MiB           1           K_next = C.shape[1]
   757   1834.0 MiB      0.0 MiB           1           inputs = []
   758   1834.0 MiB      0.0 MiB           1           fused_dense = fused_scores.toarray() if hasattr(fused_scores, "toarray") else np.asarray(fused_scores)
   759                                         
   760   1931.7 MiB    -17.5 MiB           3           for c in range(K_next):
   761   1931.7 MiB      0.0 MiB           2               local_idxs = C[:, c].nonzero()[0]
   762   1931.7 MiB      0.0 MiB           2               if len(local_idxs) == 0:
   763                                                         continue
   764                                         
   765   1931.7 MiB      0.0 MiB           2               local_to_global_next = local_to_global_idx[local_idxs]
   766   1931.7 MiB      0.0 MiB         102               global_to_local_next = {g: i for i, g in enumerate(local_to_global_next)}
   767                                         
   768   1931.7 MiB      0.0 MiB           2               Y_sub = Y[:, local_idxs]
   769   1931.7 MiB      0.0 MiB           2               mention_mask = (Y_sub.sum(axis=1).A1 > 0)
   770                                         
   771   1952.9 MiB     21.2 MiB           2               X_node = X[mention_mask]
   772   1952.9 MiB      0.0 MiB           2               Y_node = Y_sub[mention_mask, :]
   773   1952.9 MiB      0.0 MiB           2               if X_node.shape[0] == 0:
   774                                                         continue
   775                                         
   776   1952.9 MiB      0.0 MiB           2               Z_node_base = Z[local_idxs, :]
   777                                         
   778   1952.9 MiB      0.0 MiB           2               fused_c = fused_dense[mention_mask, :]
   779   1952.9 MiB      0.0 MiB           2               feat_c = fused_c[:, c].ravel()
   780   1952.9 MiB      0.0 MiB           2               feat_sum = fused_c.sum(axis=1).ravel()
   781   1952.9 MiB      0.0 MiB           2               feat_max = fused_c.max(axis=1).ravel()
   782   1952.9 MiB      0.0 MiB           2               feat_node = np.vstack([feat_c, feat_sum, feat_max]).T
   783   1952.9 MiB      0.0 MiB           2               sparse_feats = csr_matrix(feat_node)
   784                                         
   785   1952.9 MiB     58.3 MiB           2               X_aug = hstack([X_node, sparse_feats], format="csr")
   786   1952.9 MiB      0.4 MiB           2               X_aug = normalize(X_aug, norm="l2", axis=1)
   787                                         
   788   1952.9 MiB      0.0 MiB           2               try:
   789   1931.5 MiB     -0.0 MiB           2                   X_node_dense = X_node.toarray()
   790                                                     except Exception:
   791                                                         X_node_dense = np.asarray(X_node)
   792                                         
   793   1931.5 MiB    -17.4 MiB           2               if X_node_dense.size == 0 or Z_node_base.size == 0:
   794                                                         label_feats = np.zeros((Z_node_base.shape[0], 3), dtype=Z_node_base.dtype)
   795                                                     else:
   796   1931.7 MiB    -17.2 MiB           2                   scores_mat = X_node_dense.dot(Z_node_base.T)
   797   1931.7 MiB    -17.5 MiB           2                   mean_per_label = scores_mat.mean(axis=0)
   798   1931.7 MiB    -17.5 MiB           2                   sum_per_label = scores_mat.sum(axis=0)
   799   1931.7 MiB    -17.5 MiB           2                   max_per_label = scores_mat.max(axis=0)
   800   1931.7 MiB    -17.5 MiB           2                   label_feats = np.vstack([mean_per_label, sum_per_label, max_per_label]).T
   801                                         
   802   1931.7 MiB    -17.5 MiB           2               Z_node_aug = np.hstack([Z_node_base, label_feats])
   803   1931.7 MiB    -17.5 MiB           2               Z_node_aug = normalize(Z_node_aug, norm="l2", axis=1)
   804                                         
   805   1931.7 MiB    -17.5 MiB           2               inputs.append((X_aug, Y_node, Z_node_aug, local_to_global_next, global_to_local_next, c))
   806                                         
   807   1914.1 MiB    -17.5 MiB           1           return inputs


running k-means on cpu..
[running kmeans]: 0it [00:00, ?it/s][running kmeans]: 0it [00:00, ?it/s, center_shift=0.000759, iteration=1, tol=0.000100][running kmeans]: 1it [00:00, 25.60it/s, center_shift=0.000000, iteration=2, tol=0.000100][running kmeans]: 2it [00:00, 49.42it/s, center_shift=0.000000, iteration=2, tol=0.000100]
Fusing Scores
Filename: /app/xmr4el/xmr/base.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   243   1896.7 MiB   1896.7 MiB           1       @profile
   244                                             def fused_predict(self, X, Z, C, alpha=0.5, batch_size=32768,
   245                                                               fusion: str = "lp_hinge", p: int = 3):
   246                                                 """Batched matcher/ranker fusion.
   247                                         
   248                                                 Parameters
   249                                                 ----------
   250                                                 X : array-like or sparse matrix
   251                                                     Query feature matrix.
   252                                                 Z : array-like or sparse matrix
   253                                                     Label embedding matrix in the fused space.
   254                                                 C : csr_matrix
   255                                                     Cluster assignment matrix for projecting label scores.
   256                                                 alpha : float, optional
   257                                                     Interpolation coefficient between matcher and ranker scores.
   258                                                 batch_size : int, optional
   259                                                     Number of (query, label) pairs processed per ranker batch.
   260                                                 fusion : {"geometric", "lp_hinge"}, optional
   261                                                     Fusion strategy. "geometric" uses the geometric mean of matcher and
   262                                                     ranker scores. "lp_hinge" performs an L-p interpolation with a
   263                                                     hinge at zero.
   264                                                 p : int, optional
   265                                                     The ``p`` parameter used when ``fusion="lp_hinge"``.
   266                                         
   267                                                 Returns
   268                                                 -------
   269                                                 csr_matrix
   270                                                     Fused cluster score matrix of shape ``(n_queries, n_clusters)``.
   271                                                 """
   272   1896.7 MiB      0.0 MiB           1           N = X.shape[0]
   273   1896.7 MiB      0.0 MiB           1           L_local = Z.shape[0]
   274                                         
   275                                                 # --- matcher (local label-level) scores ---
   276   1896.7 MiB      0.0 MiB           1           matcher_scores = csr_matrix(self.matcher_model.model.predict_proba(X), dtype=np.float32)
   277                                         
   278                                                 # --- flatten mention-local label pairs ---
   279   1896.7 MiB      0.0 MiB           1           rows_list, cols_list, matcher_flat = [], [], []
   280   1896.7 MiB      0.0 MiB        7122           for i in range(N):
   281   1896.7 MiB      0.0 MiB        7121               row = matcher_scores.getrow(i)
   282   1896.7 MiB      0.0 MiB        7121               local_idxs, scores = row.indices, row.data
   283   1896.7 MiB      0.0 MiB        7121               rows_list.extend([i] * len(local_idxs))
   284   1896.7 MiB      0.0 MiB        7121               cols_list.extend(local_idxs)
   285   1896.7 MiB      0.0 MiB        7121               matcher_flat.extend(scores)
   286                                         
   287   1896.7 MiB      0.0 MiB           1           rows_list = np.array(rows_list, dtype=np.int32)
   288   1896.7 MiB      0.0 MiB           1           cols_list = np.array(cols_list, dtype=np.int32)
   289   1896.7 MiB      0.0 MiB           1           matcher_flat = np.array(matcher_flat, dtype=np.float32)
   290                                         
   291   1896.7 MiB      0.0 MiB           1           ranker_score = np.ones(len(rows_list), dtype=np.float32)
   292                                         
   293   1896.7 MiB      0.0 MiB           1           if self.ranker_model:
   294                                                     # Transfer original mention embeddings to dense once
   295   1978.2 MiB     81.5 MiB           1               X_dense = X.toarray() if hasattr(X, "toarray") else np.asarray(X)
   296   1978.2 MiB      0.0 MiB       14243               global_idxs = np.array([self.local_to_global_idx[i] for i in cols_list])
   297                                         
   298                                                     # print(self.ranker_model.model_dict)
   299                                         
   300                                                     # detect loss type
   301   1978.2 MiB      0.0 MiB           1               first_model = next(iter(self.ranker_model.model_dict.values()))
   302   1978.2 MiB      0.0 MiB           2               is_hinge = first_model.config.get("type") == "sklearnsgdclassifier" and \
   303   1978.2 MiB      0.0 MiB           1                       first_model.config.get("kwargs", {}).get("loss") == "hinge"
   304                                         
   305   1978.2 MiB      0.0 MiB           1               num_pairs = len(rows_list)
   306   2713.2 MiB      0.0 MiB           2               for start in range(0, num_pairs, batch_size):
   307   1978.2 MiB      0.0 MiB           1                   end = min(start + batch_size, num_pairs)
   308                                         
   309   1978.2 MiB      0.0 MiB           1                   b_rows = rows_list[start:end]
   310   1978.2 MiB      0.0 MiB           1                   b_cols = cols_list[start:end]
   311   1978.2 MiB      0.0 MiB           1                   b_global = global_idxs[start:end]
   312                                         
   313                                                         # build concatenated embeddings only for this small batch
   314   2141.5 MiB    163.3 MiB           1                   X_part = X_dense[b_rows]
   315   2223.2 MiB     81.7 MiB           1                   Z_part = Z[b_cols]
   316   2549.9 MiB    326.7 MiB           1                   batch_inp = np.hstack([X_part, Z_part])
   317                                         
   318   2549.9 MiB      0.0 MiB           1                   scores_batch = np.ones(end - start, dtype=np.float32)
   319                                                         # group by global label in this batch
   320   2713.2 MiB      0.0 MiB           3                   for g in np.unique(b_global):
   321   2713.1 MiB      0.0 MiB           2                       mask = (b_global == g)
   322   2713.1 MiB      0.0 MiB           2                       if g not in self.ranker_model.model_dict:
   323                                                                 continue
   324   2713.1 MiB      0.0 MiB           2                       mdl = self.ranker_model.model_dict[g]
   325   2713.2 MiB    163.3 MiB           2                       sub_inp = batch_inp[mask]
   326                                         
   327   2713.2 MiB      0.0 MiB           2                       try:
   328   2713.2 MiB      0.0 MiB           2                           if is_hinge:
   329   2713.2 MiB      0.0 MiB           2                               score = np.clip(expit(mdl.decision_function(sub_inp)), 1e-6, 1.0)
   330                                                                 else:
   331                                                                     score = np.clip(mdl.predict_proba(sub_inp)[:, 1], 1e-6, 1.0)
   332   2713.2 MiB      0.0 MiB           2                           scores_batch[mask] = score
   333                                                                 
   334                                                             except Exception:
   335                                                                 scores_batch[mask] = 1.0
   336                                         
   337   2713.2 MiB      0.0 MiB           1                   ranker_score[start:end] = scores_batch
   338                                                         
   339                                                     else:
   340   2713.2 MiB      0.0 MiB           1                   alpha = 0
   341                                                         
   342   2713.2 MiB      0.0 MiB           1           self.alpha = alpha
   343                                         
   344   2713.2 MiB      0.0 MiB           1           if fusion == "lp_hinge":
   345   2713.2 MiB      0.0 MiB           3               fused = ((1 - self.alpha) * (matcher_flat ** p) +
   346   2713.2 MiB      0.0 MiB           2                        self.alpha * (ranker_score ** p)) ** (1.0 / p)
   347   2713.2 MiB      0.0 MiB           1               fused = np.maximum(fused, 0.0)
   348                                                 else:
   349                                                     fused = (matcher_flat ** (1 - self.alpha)) * (ranker_score ** self.alpha)
   350                                         
   351                                                 # --- build local-label fused matrix ---
   352   2713.2 MiB      0.0 MiB           1           entity_fused = csr_matrix((fused, (rows_list, cols_list)), shape=(N, L_local), dtype=np.float32)
   353                                         
   354                                                 # --- project to clusters ---
   355   2713.2 MiB      0.0 MiB           1           cluster_fused = entity_fused.dot(C)
   356   2713.2 MiB      0.0 MiB           1           return csr_matrix(cluster_fused)


Filename: /app/xmr4el/xmr/base.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   358   1914.8 MiB   1914.8 MiB           1       @profile
   359                                             def train(self, X_train, Y_train, Z_train, local_to_global, global_to_local):
   360                                                 """
   361                                                     X_train: X_processed
   362                                                     Y_train, Y_binazier
   363                                                     Z, Pifa embeddings
   364                                                 """
   365                                                 
   366                                                 # --- Ensure Z is in fused space ---
   367   1914.8 MiB      0.0 MiB           1           Z_train = normalize(Z_train, norm="l2", axis=1) 
   368   1914.8 MiB      0.0 MiB           1           self.label_embeddings = Z_train
   369   1914.8 MiB      0.0 MiB           1           del Z_train
   370                                                 
   371   1914.8 MiB      0.0 MiB           1           self.global_to_local_idx = global_to_local
   372   1914.8 MiB      0.0 MiB           1           self.local_to_global_idx = np.array(local_to_global, dtype=int)
   373                                                 
   374   1914.8 MiB      0.0 MiB           1           del global_to_local
   375                                                 
   376                                                 # print("Clustering")
   377                                                 # Make the Clustering
   378   1914.8 MiB      0.0 MiB           1           cluster_model = Clustering()
   379   1928.9 MiB     14.1 MiB           2           cluster_model.train(Z=self.label_embeddings, 
   380   1914.8 MiB      0.0 MiB           1                               local_to_global_idx=self.local_to_global_idx,
   381   1914.8 MiB      0.0 MiB           1                               min_leaf_size=self.min_leaf_size,
   382   1914.8 MiB      0.0 MiB           1                               max_leaf_size=self.max_leaf_size,
   383   1914.8 MiB      0.0 MiB           1                               clustering_config=self.clustering_config,
   384   1914.8 MiB      0.0 MiB           1                               dtype=np.float32
   385                                                                     ) # Hardcoded
   386                                                 
   387   1928.9 MiB      0.0 MiB           1           if cluster_model.is_empty:
   388                                                     return
   389                                                 
   390   1928.9 MiB      0.0 MiB           1           self.cluster_model = cluster_model
   391   1928.9 MiB      0.0 MiB           1           del cluster_model
   392   1928.9 MiB      0.0 MiB           1           gc.collect()
   393                                                 
   394                                                 # Retrieve C
   395   1928.9 MiB      0.0 MiB           1           C = self.cluster_model.c_node
   396   1928.9 MiB      0.0 MiB           1           cluster_labels = np.asarray(C.argmax(axis=1)).flatten()
   397                                             
   398                                                 # Make the Matcher
   399   1928.9 MiB      0.0 MiB           1           matcher_model = Matcher()  
   400   1928.9 MiB    -21.5 MiB           2           matcher_model.train(X_train, 
   401   1928.9 MiB      0.0 MiB           1                               Y_train, 
   402   1928.9 MiB      0.0 MiB           1                               local_to_global_idx=self.local_to_global_idx, 
   403   1928.9 MiB      0.0 MiB           1                               global_to_local_idx=self.global_to_local_idx, 
   404   1928.9 MiB      0.0 MiB           1                               C=C,
   405   1928.9 MiB      0.0 MiB           1                               matcher_config=self.matcher_config,
   406   1928.9 MiB      0.0 MiB           1                               dtype=np.float32
   407                                                                     )     
   408                                                  
   409   1907.4 MiB    -21.5 MiB           1           self.matcher_model = matcher_model 
   410   1907.4 MiB      0.0 MiB           1           del matcher_model
   411   1911.8 MiB      4.3 MiB           1           gc.collect()
   412                                                 
   413   1911.8 MiB      0.0 MiB           1           train_ranker = self.ranker_every_layer or self.is_last_layer
   414                                                 
   415   1912.3 MiB      0.0 MiB           2           def _topb_sparse(P: np.ndarray, b: int) -> csr_matrix:
   416                                                     # P: (n x K_or_L) dense proba; returns (n x K_or_L) CSR 0/1 mask of top-b per row
   417   1912.3 MiB      0.0 MiB           1               n, K = P.shape
   418   1912.3 MiB      0.0 MiB           1               b = max(1, min(b, K))
   419   1912.3 MiB      0.0 MiB           1               idx_part = np.argpartition(P, K - b, axis=1)[:, -b:]
   420   1912.3 MiB      0.0 MiB           1               rows = np.repeat(np.arange(n, dtype=np.int32), b)
   421   1912.3 MiB      0.0 MiB           1               cols = idx_part.ravel()
   422   1912.3 MiB      0.0 MiB           1               data = np.ones(n * b, dtype=np.int8)
   423   1912.6 MiB      0.4 MiB           1               return csr_matrix((data, (rows, cols)), shape=(n, K))
   424                                                 
   425   1911.8 MiB      0.0 MiB           1           if train_ranker:
   426                                                 
   427   1911.8 MiB      0.0 MiB           1               M_TFN = self.matcher_model.m_node
   428   1911.8 MiB      0.0 MiB           1               M_MAN = None
   429                                                 
   430   1911.8 MiB      0.0 MiB           1               if self.is_last_layer:
   431   1912.3 MiB      0.5 MiB           1                   P = self.matcher_model.predict_proba(X_train)
   432   1912.6 MiB      0.0 MiB           1                   M_MAN = _topb_sparse(P, b=5)
   433                                                     
   434                                                     # print("Ranker")
   435   1912.6 MiB      0.0 MiB           1               ranker_model = Ranker()
   436   1912.6 MiB    -15.9 MiB           2               ranker_model.train(X_train, 
   437   1912.6 MiB      0.0 MiB           1                                   Y_train, 
   438   1912.6 MiB      0.0 MiB           1                                   self.label_embeddings, 
   439   1912.6 MiB      0.0 MiB           1                                   M_TFN, 
   440   1912.6 MiB      0.0 MiB           1                                   M_MAN, 
   441   1912.6 MiB      0.0 MiB           1                                   cluster_labels,
   442   1912.6 MiB      0.0 MiB           1                                   local_to_global_idx=self.local_to_global_idx,
   443   1912.6 MiB      0.0 MiB           1                                   layer=self.layer,
   444   1912.6 MiB      0.0 MiB           1                                   n_label_workers=self.n_workers,
   445   1912.6 MiB      0.0 MiB           1                                   ranker_config=self.ranker_config,
   446   1912.6 MiB      0.0 MiB           1                                   cur_config=self.cur_config
   447                                                                         )
   448                                                     
   449   1896.7 MiB    -15.9 MiB           1               self.ranker_model = ranker_model
   450   1896.7 MiB      0.0 MiB           1               del ranker_model
   451                                                 else:
   452                                                     self.ranker_model = None
   453                                                     
   454   1896.7 MiB      0.0 MiB           1           gc.collect()
   455                                                 
   456   1896.7 MiB      0.0 MiB           1           print("Fusing Scores")
   457   1896.7 MiB      0.0 MiB           1           if not self.is_last_layer:
   458                                                     cluster_scores = self.matcher_model.predict_proba(X_train)
   459                                                     self.fused_scores = csr_matrix(np.maximum(cluster_scores, 0.0))
   460                                                 else:
   461   1896.7 MiB      0.0 MiB           1               I_L = sp_eye(self.label_embeddings.shape[0], format="csr", dtype=np.float32)
   462   1896.7 MiB     -0.0 MiB           1               self.fused_scores = self.fused_predict(X_train, self.label_embeddings, I_L, alpha=0.5, fusion="lp_hinge", p=3)


Filename: /app/xmr4el/xmr/base.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   749   1896.7 MiB   1896.7 MiB           1       @profile
   750                                             def prepare_layer(self, X, Y, Z, C, fused_scores, local_to_global_idx):
   751                                                 """
   752                                                 Returns a list of tuples, one per (non-empty) cluster c:
   753                                                 (X_aug, Y_node, Z_node_aug, local_to_global_next, global_to_local_next, c)
   754                                                 where `c` is the *cluster id* in the parent.
   755                                                 """
   756   1896.7 MiB      0.0 MiB           1           K_next = C.shape[1]
   757   1896.7 MiB      0.0 MiB           1           inputs = []
   758   1896.7 MiB      0.0 MiB           1           fused_dense = fused_scores.toarray() if hasattr(fused_scores, "toarray") else np.asarray(fused_scores)
   759                                         
   760   2131.2 MiB      0.0 MiB           3           for c in range(K_next):
   761   1923.3 MiB      0.0 MiB           2               local_idxs = C[:, c].nonzero()[0]
   762   1923.3 MiB      0.0 MiB           2               if len(local_idxs) == 0:
   763                                                         continue
   764                                         
   765   1923.3 MiB      0.0 MiB           2               local_to_global_next = local_to_global_idx[local_idxs]
   766   1923.3 MiB      0.0 MiB         102               global_to_local_next = {g: i for i, g in enumerate(local_to_global_next)}
   767                                         
   768   1923.3 MiB      0.0 MiB           2               Y_sub = Y[:, local_idxs]
   769   1923.3 MiB      0.0 MiB           2               mention_mask = (Y_sub.sum(axis=1).A1 > 0)
   770                                         
   771   2001.1 MiB     77.9 MiB           2               X_node = X[mention_mask]
   772   2001.1 MiB      0.0 MiB           2               Y_node = Y_sub[mention_mask, :]
   773   2001.1 MiB      0.0 MiB           2               if X_node.shape[0] == 0:
   774                                                         continue
   775                                         
   776   2001.1 MiB      0.0 MiB           2               Z_node_base = Z[local_idxs, :]
   777                                         
   778   2001.1 MiB      0.0 MiB           2               fused_c = fused_dense[mention_mask, :]
   779   2001.1 MiB      0.0 MiB           2               feat_c = fused_c[:, c].ravel()
   780   2001.1 MiB      0.0 MiB           2               feat_sum = fused_c.sum(axis=1).ravel()
   781   2001.1 MiB      0.0 MiB           2               feat_max = fused_c.max(axis=1).ravel()
   782   2001.1 MiB      0.0 MiB           2               feat_node = np.vstack([feat_c, feat_sum, feat_max]).T
   783   2001.1 MiB      0.0 MiB           2               sparse_feats = csr_matrix(feat_node)
   784                                         
   785   2079.3 MiB    104.3 MiB           2               X_aug = hstack([X_node, sparse_feats], format="csr")
   786   2079.3 MiB      0.5 MiB           2               X_aug = normalize(X_aug, norm="l2", axis=1)
   787                                         
   788   2079.3 MiB      0.0 MiB           2               try:
   789   2131.2 MiB     51.8 MiB           2                   X_node_dense = X_node.toarray()
   790                                                     except Exception:
   791                                                         X_node_dense = np.asarray(X_node)
   792                                         
   793   2131.2 MiB      0.0 MiB           2               if X_node_dense.size == 0 or Z_node_base.size == 0:
   794                                                         label_feats = np.zeros((Z_node_base.shape[0], 3), dtype=Z_node_base.dtype)
   795                                                     else:
   796   2131.2 MiB      0.0 MiB           2                   scores_mat = X_node_dense.dot(Z_node_base.T)
   797   2131.2 MiB      0.0 MiB           2                   mean_per_label = scores_mat.mean(axis=0)
   798   2131.2 MiB      0.0 MiB           2                   sum_per_label = scores_mat.sum(axis=0)
   799   2131.2 MiB      0.0 MiB           2                   max_per_label = scores_mat.max(axis=0)
   800   2131.2 MiB      0.0 MiB           2                   label_feats = np.vstack([mean_per_label, sum_per_label, max_per_label]).T
   801                                         
   802   2131.2 MiB      0.0 MiB           2               Z_node_aug = np.hstack([Z_node_base, label_feats])
   803   2131.2 MiB      0.0 MiB           2               Z_node_aug = normalize(Z_node_aug, norm="l2", axis=1)
   804                                         
   805   2131.2 MiB      0.0 MiB           2               inputs.append((X_aug, Y_node, Z_node_aug, local_to_global_next, global_to_local_next, c))
   806                                         
   807   2131.2 MiB      0.0 MiB           1           return inputs


Filename: /app/xmr4el/xmr/base.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   809    909.3 MiB    909.3 MiB           1       @profile
   810                                             def train(self, X_train, Y_train, Z_train, local_to_global, global_to_local):
   811                                                 """
   812                                                 Train multiple layers of MLModel; intermediate models are saved in a
   813                                                 temporary folder which is automatically deleted at the end of training.
   814                                                 """
   815    909.3 MiB      0.0 MiB           1           inputs = [(X_train, Y_train, Z_train, local_to_global, global_to_local)]
   816                                         
   817                                                 # Use TemporaryDirectory to ensure cleanup
   818   1968.0 MiB      0.0 MiB           2           with tempfile.TemporaryDirectory(prefix="ml_store_") as temp_dir:
   819    909.3 MiB      0.0 MiB           1               self.ml_dir = Path(temp_dir)
   820    909.3 MiB      0.0 MiB           1               self.hmodel = []
   821    909.3 MiB      0.0 MiB           1               child_index_map = []
   822                                         
   823    909.3 MiB      0.0 MiB           1               if self.ranker_every_layer:
   824    909.3 MiB      0.0 MiB           1                   ranker_flag = True
   825                                                     else:
   826                                                         ranker_flag = False
   827                                         
   828   1968.0 MiB      0.0 MiB           3               for layer in range(self.layers):
   829   1760.2 MiB      0.0 MiB           2                   next_inputs = []
   830   1760.2 MiB      0.0 MiB           2                   ml_list = []
   831   1760.2 MiB      0.0 MiB           2                   layer_failed = False
   832   1760.2 MiB      0.0 MiB           2                   is_last_layer = False
   833   1760.2 MiB      0.0 MiB           2                   cluster_map_for_layer = []
   834                                         
   835                                                         # Optional: adjust cluster size
   836   1760.2 MiB      0.0 MiB           2                   if self.cut_half_cluster and layer > 0:
   837   1760.2 MiB      0.0 MiB           2                       self.clustering_config["kwargs"]["n_clusters"] = max(
   838   1760.2 MiB      0.0 MiB           1                           2, self.clustering_config["kwargs"]["n_clusters"] // 2
   839                                                             )
   840                                                         
   841   2028.1 MiB      0.0 MiB           5                   for parent_idx, (X_node, Y_node, Z_node, local_to_label_node, global_to_local_node) in enumerate(inputs):
   842                                                             
   843   1914.8 MiB      0.0 MiB           3                       if layer == self.layers - 1:
   844   1914.8 MiB      0.0 MiB           2                           ranker_flag = True
   845   1914.8 MiB      0.0 MiB           2                           is_last_layer = True
   846                                                             
   847   1914.8 MiB      0.0 MiB           6                       ml = MLModel(
   848   1914.8 MiB      0.0 MiB           3                           clustering_config=self.clustering_config,
   849   1914.8 MiB      0.0 MiB           3                           matcher_config=self.matcher_config,
   850   1914.8 MiB      0.0 MiB           3                           ranker_config=self.ranker_config,
   851   1914.8 MiB      0.0 MiB           3                           cur_config=self.cur_config,
   852   1914.8 MiB      0.0 MiB           3                           min_leaf_size=self.min_leaf_size,
   853   1914.8 MiB      0.0 MiB           3                           max_leaf_size=self.max_leaf_size,
   854   1914.8 MiB      0.0 MiB           3                           ranker_every_layer= ranker_flag,
   855   1914.8 MiB      0.0 MiB           3                           is_last_layer=is_last_layer,
   856   1914.8 MiB      0.0 MiB           3                           layer=layer,
   857   1914.8 MiB      0.0 MiB           3                           n_workers=self.n_workers,
   858                                                             )
   859                                         
   860   1914.8 MiB    605.4 MiB           6                       ml.train(
   861   1914.8 MiB      0.0 MiB           3                           X_train=X_node,
   862   1914.8 MiB      0.0 MiB           3                           Y_train=Y_node,
   863   1914.8 MiB      0.0 MiB           3                           Z_train=Z_node,
   864   1914.8 MiB      0.0 MiB           3                           local_to_global=local_to_label_node,
   865   1914.8 MiB      0.0 MiB           3                           global_to_local=global_to_local_node
   866                                                             )
   867                                         
   868   1896.7 MiB    -18.1 MiB           3                       if ml.is_empty:
   869                                                                 layer_failed = True
   870                                                                 break
   871                                         
   872   1896.7 MiB      0.0 MiB           3                       C = ml.cluster_model.c_node
   873   1896.7 MiB      0.0 MiB           3                       fused_scores = ml.fused_scores
   874                                         
   875                                                             # Prepare inputs for next layer
   876   2027.4 MiB    511.3 MiB           6                       raw_children = self.prepare_layer(
   877   1896.7 MiB    -17.4 MiB           3                           X=X_node,
   878   1896.7 MiB      0.0 MiB           3                           Y=Y_node,
   879   1896.7 MiB      0.0 MiB           3                           Z=Z_node,
   880   1896.7 MiB      0.0 MiB           3                           C=C,
   881   1896.7 MiB      0.0 MiB           3                           fused_scores=fused_scores,
   882   1896.7 MiB      0.0 MiB           3                           local_to_global_idx=local_to_label_node
   883                                                             )
   884                                                             
   885   2027.4 MiB      0.0 MiB           3                       cluster_to_child = {}
   886   2027.4 MiB      0.0 MiB           9                       for child in raw_children:
   887   2027.4 MiB      0.0 MiB           6                           *payload, c_id = child
   888   2027.4 MiB      0.0 MiB           6                           child_abs_idx = len(next_inputs)
   889   2027.4 MiB      0.0 MiB           6                           next_inputs.append(tuple(payload))
   890   2027.4 MiB      0.0 MiB           6                           cluster_to_child[int(c_id)] = child_abs_idx
   891                                                             
   892                                                             # Save model in temporary folder with unique name
   893   2027.5 MiB      0.4 MiB           3                       ml_path = self.save_ml_temp(ml, f"{layer}_{uuid4()}")
   894   2028.1 MiB      1.7 MiB           3                       del ml
   895   2028.1 MiB      0.0 MiB           3                       ml_list.append(ml_path)
   896   2028.1 MiB      0.0 MiB           3                       cluster_map_for_layer.append(cluster_to_child)
   897                                         
   898   2028.1 MiB      0.0 MiB           2                   if layer_failed:
   899                                                             break
   900                                         
   901   2028.1 MiB      0.0 MiB           2                   self.hmodel.append(ml_list)
   902   2028.1 MiB      0.0 MiB           2                   child_index_map.append(cluster_map_for_layer)
   903                                         
   904   1968.0 MiB    -60.1 MiB           2                   inputs = next_inputs
   905   1968.0 MiB      0.0 MiB           2                   del ml_list
   906   1968.0 MiB      0.0 MiB           2                   gc.collect()
   907                                         
   908                                                     # Reload all models for final hmodel
   909   1968.0 MiB      0.0 MiB           1               new_hmodel_list = []
   910   1968.0 MiB      0.0 MiB           3               for model_list in self.hmodel:
   911   1968.0 MiB      0.0 MiB           5                   loaded = [MLModel.load(p) for p in model_list]
   912   1968.0 MiB      0.0 MiB           2                   new_hmodel_list.append(loaded)
   913   1968.0 MiB      0.0 MiB           1               self.hmodel = new_hmodel_list
   914                                         
   915   1968.0 MiB      0.0 MiB           1               self._child_index_map = child_index_map
   916                                                     
   917   1968.0 MiB      0.0 MiB           1               return self.hmodel


Filename: /app/xmr4el/xmr/model.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   230    553.1 MiB    553.1 MiB           1       @profile
   231                                             def train(self, X_text, Y_text):
   232    909.3 MiB    356.2 MiB           1           self.X, self.Y, self.Z = self._fit(X_text=X_text, Y_text=Y_text)
   233                                         
   234    909.3 MiB      0.0 MiB           1           n_labels = self.Z.shape[0]
   235    909.3 MiB      0.0 MiB           1           local_to_global = np.arange(n_labels, dtype=int)
   236    909.3 MiB      0.0 MiB         201           global_to_local = {g: i for i, g in enumerate(local_to_global)}
   237                                         
   238    909.3 MiB      0.0 MiB           2           hml = HierarchicaMLModel(
   239    909.3 MiB      0.0 MiB           1               clustering_config=self.clustering_config,
   240    909.3 MiB      0.0 MiB           1               matcher_config=self.matcher_config,
   241    909.3 MiB      0.0 MiB           1               ranker_config=self.ranker_config,
   242    909.3 MiB      0.0 MiB           1               cur_config=self.cur_config,
   243    909.3 MiB      0.0 MiB           1               min_leaf_size=self.min_leaf_size,
   244    909.3 MiB      0.0 MiB           1               max_leaf_size=self.max_leaf_size,
   245    909.3 MiB      0.0 MiB           1               n_workers=self.n_workers,
   246    909.3 MiB      0.0 MiB           1               cut_half_cluster=self.cut_half_cluster,
   247    909.3 MiB      0.0 MiB           1               ranker_every_layer=self.ranker_every_layer,
   248    909.3 MiB      0.0 MiB           1               layer=self.depth
   249                                                 )
   250                                         
   251   1594.4 MiB    685.1 MiB           2           hml.train(
   252    909.3 MiB      0.0 MiB           1               X_train=self.X,
   253    909.3 MiB      0.0 MiB           1               Y_train=self.Y,
   254    909.3 MiB      0.0 MiB           1               Z_train=self.Z,
   255    909.3 MiB      0.0 MiB           1               local_to_global=local_to_global,
   256    909.3 MiB      0.0 MiB           1               global_to_local=global_to_local
   257                                                 )
   258                                         
   259   1594.4 MiB      0.0 MiB           1           self.model = hml
   260                                                 
   261   1594.4 MiB      0.0 MiB           1           self.initial_labels = self.temp_var.load_model_temp(self.initial_labels)
   262   1594.4 MiB      0.0 MiB           1           self.training_set = self.temp_var.load_model_temp(self.training_set)
   263                                                 
   264   1594.4 MiB      0.0 MiB           1           self.temp_var.delete_model_temp()


102.93351864814758 secs of running
mprof: Sampling memory every 0.1s
running new process
