{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9078e77",
   "metadata": {},
   "source": [
    "# How to train a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2ec229",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from xmr4el.featurization.preprocessor import Preprocessor\n",
    "from xmr4el.xmr.pipeline import XMRPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6f1a2a",
   "metadata": {},
   "source": [
    "# Initialize the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36439e7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "onnx_directory = \"test/test_data/processed/vectorizer/biobert_onnx_cpu.onnx\"\n",
    "\n",
    "n_features = 500\n",
    "min_leaf_size = 10\n",
    "depth = 3\n",
    "\n",
    "vectorizer_config = {\n",
    "    \"type\": \"tfidf\", \n",
    "    \"kwargs\": {\"max_features\": n_features}\n",
    "}\n",
    "\n",
    "transformer_config = {\n",
    "    \"type\": \"biobert\",\n",
    "    \"kwargs\": {\"batch_size\": 400, \"onnx_directory\": onnx_directory},\n",
    "}\n",
    "    \n",
    "clustering_config = {\n",
    "    \"type\": \"sklearnminibatchkmeans\",\n",
    "    \"kwargs\": {\"random_state\": 0},\n",
    "}\n",
    "\n",
    "classifier_config = {\n",
    "    \"type\": \"sklearnlogisticregression\",\n",
    "    \"kwargs\": {\"n_jobs\": -1, \"random_state\": 0},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59764ef7",
   "metadata": {},
   "source": [
    "# Get the data from the files\n",
    "\n",
    "truncate_data has as purpose to test the models in a minimal data environment, if no need of truncate just don't include it in the Preprocessor().load_data_labels_from_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b343161",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "training_file = os.path.join(os.getcwd(), \"test/test_data/train/disease/train_Disease_100.txt\")\n",
    "labels_file = os.path.join(os.getcwd(), \"data/raw/mesh_data/medic/labels.txt\")\n",
    "\n",
    "truncate_data = 16\n",
    "\n",
    "train_data = Preprocessor().load_data_labels_from_file(\n",
    "    train_filepath=training_file,\n",
    "    labels_filepath=labels_file,\n",
    "    truncate_data=truncate_data\n",
    ")\n",
    "\n",
    "Y_train = train_data[\"labels_matrix\"] # csr.matrix\n",
    "X_train = train_data[\"corpus\"] # List\n",
    "label_enconder = train_data[\"label_encoder\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777f57b0",
   "metadata": {},
   "source": [
    "# Execute the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c879b4ca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "htree = XMRPipeline.execute_pipeline(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    label_enconder, # New\n",
    "    vectorizer_config,\n",
    "    transformer_config,\n",
    "    clustering_config,\n",
    "    classifier_config,\n",
    "    n_features=n_features,  # Number of Features\n",
    "    max_n_clusters=16,\n",
    "    min_n_clusters=2, # Changed to 2, must be 6\n",
    "    min_leaf_size=min_leaf_size,\n",
    "    depth=depth,\n",
    "    dtype=np.float32,\n",
    ")\n",
    "\n",
    "# Save the tree\n",
    "save_dir = os.path.join(os.getcwd(), \"test/test_data/saved_trees\")  # Ensure this path is correct and writable\n",
    "htree.save(save_dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
